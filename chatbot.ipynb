{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.notebook import tqdm, trange\n\nfrom pathlib import Path\n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:48:59.301299Z","iopub.execute_input":"2023-03-12T19:48:59.302279Z","iopub.status.idle":"2023-03-12T19:49:12.607393Z","shell.execute_reply.started":"2023-03-12T19:48:59.302234Z","shell.execute_reply":"2023-03-12T19:49:12.606215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndata= pd.read_csv('/kaggle/input/harrypotter/HarryPotter1.csv', sep=';')\ndata.head() ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.610050Z","iopub.execute_input":"2023-03-12T19:49:12.610345Z","iopub.status.idle":"2023-03-12T19:49:12.655843Z","shell.execute_reply.started":"2023-03-12T19:49:12.610312Z","shell.execute_reply":"2023-03-12T19:49:12.654861Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    Character                                           Sentence\n0  Dumbledore  I should've known that you would be here, Prof...\n1  McGonagall                Good evening, Professor Dumbledore.\n2  McGonagall                        Are the rumors true, Albus?\n3  Dumbledore                          I'm afraid so, professor.\n4  Dumbledore                              The good and the bad.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Character</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dumbledore</td>\n      <td>I should've known that you would be here, Prof...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>McGonagall</td>\n      <td>Good evening, Professor Dumbledore.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>McGonagall</td>\n      <td>Are the rumors true, Albus?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dumbledore</td>\n      <td>I'm afraid so, professor.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dumbledore</td>\n      <td>The good and the bad.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Count number of lines belonging to a character\nsum(data['Character']=='Harry')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.657687Z","iopub.execute_input":"2023-03-12T19:49:12.658389Z","iopub.status.idle":"2023-03-12T19:49:12.668729Z","shell.execute_reply.started":"2023-03-12T19:49:12.658347Z","shell.execute_reply":"2023-03-12T19:49:12.667751Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"155"},"metadata":{}}]},{"cell_type":"code","source":"# Check the length of the dataframe\nlen(data)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.672282Z","iopub.execute_input":"2023-03-12T19:49:12.672664Z","iopub.status.idle":"2023-03-12T19:49:12.680627Z","shell.execute_reply.started":"2023-03-12T19:49:12.672636Z","shell.execute_reply":"2023-03-12T19:49:12.678993Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"1587"},"metadata":{}}]},{"cell_type":"code","source":"# Change the column names\ndata.rename(columns={'Character':'name','Sentence':'line'},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.682250Z","iopub.execute_input":"2023-03-12T19:49:12.682948Z","iopub.status.idle":"2023-03-12T19:49:12.689237Z","shell.execute_reply.started":"2023-03-12T19:49:12.682899Z","shell.execute_reply":"2023-03-12T19:49:12.688179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create a context dataframe for one character only\nCharacter_name='Harry'\ncontexted = []\n\n# context window of size 7\nn = 7\n\nfor i in data[data.name == Character_name].index:\n    if i < n:\n        continue\n    row = []\n    prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n    for j in range(i, prev, -1):\n        row.append(data.line[j])\n        contexted.append(row)\n\ncolumns = ['response', 'context'] \ncolumns = columns + ['context/' + str(i) for i in range(n - 1)]\n\ndf = pd.DataFrame.from_records(contexted, columns=columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.691002Z","iopub.execute_input":"2023-03-12T19:49:12.691737Z","iopub.status.idle":"2023-03-12T19:49:12.728757Z","shell.execute_reply.started":"2023-03-12T19:49:12.691682Z","shell.execute_reply":"2023-03-12T19:49:12.727729Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                             response  \\\n0                  Yes, Aunt Petunia.   \n1                  Yes, Aunt Petunia.   \n2                  Yes, Aunt Petunia.   \n3                  Yes, Aunt Petunia.   \n4                  Yes, Aunt Petunia.   \n...                               ...   \n1235  I'm not going home. Not really.   \n1236  I'm not going home. Not really.   \n1237  I'm not going home. Not really.   \n1238  I'm not going home. Not really.   \n1239  I'm not going home. Not really.   \n\n                                                context  \\\n0     Why don't you just cook the breakfast, and try...   \n1     Why don't you just cook the breakfast, and try...   \n2     Why don't you just cook the breakfast, and try...   \n3     Why don't you just cook the breakfast, and try...   \n4     Why don't you just cook the breakfast, and try...   \n...                                                 ...   \n1235        Feels strange to be going home, doesn't it?   \n1236        Feels strange to be going home, doesn't it?   \n1237        Feels strange to be going home, doesn't it?   \n1238        Feels strange to be going home, doesn't it?   \n1239        Feels strange to be going home, doesn't it?   \n\n                                              context/0  \\\n0                                  Happy birthday, son.   \n1                                  Happy birthday, son.   \n2                                  Happy birthday, son.   \n3                                  Happy birthday, son.   \n4                                  Happy birthday, son.   \n...                                                 ...   \n1235  I do. But your cousin don't, do he? Eh? Off yo...   \n1236  I do. But your cousin don't, do he? Eh? Off yo...   \n1237  I do. But your cousin don't, do he? Eh? Off yo...   \n1238  I do. But your cousin don't, do he? Eh? Off yo...   \n1239  I do. But your cousin don't, do he? Eh? Off yo...   \n\n                                              context/1  \\\n0                      Here he comes, the birthday boy.   \n1                      Here he comes, the birthday boy.   \n2                      Here he comes, the birthday boy.   \n3                      Here he comes, the birthday boy.   \n4                      Here he comes, the birthday boy.   \n...                                                 ...   \n1235  But Hagrid, we're not allowed to do magic away...   \n1236  But Hagrid, we're not allowed to do magic away...   \n1237  But Hagrid, we're not allowed to do magic away...   \n1238  But Hagrid, we're not allowed to do magic away...   \n1239  But Hagrid, we're not allowed to do magic away...   \n\n                                              context/2  \\\n0                               We're going to the zoo!   \n1                               We're going to the zoo!   \n2                               We're going to the zoo!   \n3                               We're going to the zoo!   \n4                               We're going to the zoo!   \n...                                                 ...   \n1235  Oh, listen, Harry, if that dolt of a cousin of...   \n1236  Oh, listen, Harry, if that dolt of a cousin of...   \n1237  Oh, listen, Harry, if that dolt of a cousin of...   \n1238  Oh, listen, Harry, if that dolt of a cousin of...   \n1239  Oh, listen, Harry, if that dolt of a cousin of...   \n\n                      context/3         context/4         context/5  \n0              Wake up, cousin!              Now!       Up. Get up!  \n1              Wake up, cousin!              Now!       Up. Get up!  \n2              Wake up, cousin!              Now!       Up. Get up!  \n3              Wake up, cousin!              Now!       Up. Get up!  \n4              Wake up, cousin!              Now!       Up. Get up!  \n...                         ...               ...               ...  \n1235  Oh. Go on...on with you.   Thanks, Hagrid.   This is for you.  \n1236  Oh. Go on...on with you.   Thanks, Hagrid.   This is for you.  \n1237  Oh. Go on...on with you.   Thanks, Hagrid.   This is for you.  \n1238  Oh. Go on...on with you.   Thanks, Hagrid.   This is for you.  \n1239  Oh. Go on...on with you.   Thanks, Hagrid.   This is for you.  \n\n[1240 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Yes, Aunt Petunia.</td>\n      <td>Why don't you just cook the breakfast, and try...</td>\n      <td>Happy birthday, son.</td>\n      <td>Here he comes, the birthday boy.</td>\n      <td>We're going to the zoo!</td>\n      <td>Wake up, cousin!</td>\n      <td>Now!</td>\n      <td>Up. Get up!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yes, Aunt Petunia.</td>\n      <td>Why don't you just cook the breakfast, and try...</td>\n      <td>Happy birthday, son.</td>\n      <td>Here he comes, the birthday boy.</td>\n      <td>We're going to the zoo!</td>\n      <td>Wake up, cousin!</td>\n      <td>Now!</td>\n      <td>Up. Get up!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yes, Aunt Petunia.</td>\n      <td>Why don't you just cook the breakfast, and try...</td>\n      <td>Happy birthday, son.</td>\n      <td>Here he comes, the birthday boy.</td>\n      <td>We're going to the zoo!</td>\n      <td>Wake up, cousin!</td>\n      <td>Now!</td>\n      <td>Up. Get up!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yes, Aunt Petunia.</td>\n      <td>Why don't you just cook the breakfast, and try...</td>\n      <td>Happy birthday, son.</td>\n      <td>Here he comes, the birthday boy.</td>\n      <td>We're going to the zoo!</td>\n      <td>Wake up, cousin!</td>\n      <td>Now!</td>\n      <td>Up. Get up!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yes, Aunt Petunia.</td>\n      <td>Why don't you just cook the breakfast, and try...</td>\n      <td>Happy birthday, son.</td>\n      <td>Here he comes, the birthday boy.</td>\n      <td>We're going to the zoo!</td>\n      <td>Wake up, cousin!</td>\n      <td>Now!</td>\n      <td>Up. Get up!</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1235</th>\n      <td>I'm not going home. Not really.</td>\n      <td>Feels strange to be going home, doesn't it?</td>\n      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n      <td>But Hagrid, we're not allowed to do magic away...</td>\n      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n      <td>Oh. Go on...on with you.</td>\n      <td>Thanks, Hagrid.</td>\n      <td>This is for you.</td>\n    </tr>\n    <tr>\n      <th>1236</th>\n      <td>I'm not going home. Not really.</td>\n      <td>Feels strange to be going home, doesn't it?</td>\n      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n      <td>But Hagrid, we're not allowed to do magic away...</td>\n      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n      <td>Oh. Go on...on with you.</td>\n      <td>Thanks, Hagrid.</td>\n      <td>This is for you.</td>\n    </tr>\n    <tr>\n      <th>1237</th>\n      <td>I'm not going home. Not really.</td>\n      <td>Feels strange to be going home, doesn't it?</td>\n      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n      <td>But Hagrid, we're not allowed to do magic away...</td>\n      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n      <td>Oh. Go on...on with you.</td>\n      <td>Thanks, Hagrid.</td>\n      <td>This is for you.</td>\n    </tr>\n    <tr>\n      <th>1238</th>\n      <td>I'm not going home. Not really.</td>\n      <td>Feels strange to be going home, doesn't it?</td>\n      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n      <td>But Hagrid, we're not allowed to do magic away...</td>\n      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n      <td>Oh. Go on...on with you.</td>\n      <td>Thanks, Hagrid.</td>\n      <td>This is for you.</td>\n    </tr>\n    <tr>\n      <th>1239</th>\n      <td>I'm not going home. Not really.</td>\n      <td>Feels strange to be going home, doesn't it?</td>\n      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n      <td>But Hagrid, we're not allowed to do magic away...</td>\n      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n      <td>Oh. Go on...on with you.</td>\n      <td>Thanks, Hagrid.</td>\n      <td>This is for you.</td>\n    </tr>\n  </tbody>\n</table>\n<p>1240 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split data to train and test datasets\n# Train the model on the training set and test the model with the test set\ntrn_df, val_df = train_test_split(df, test_size=0.1)\ntrn_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.730161Z","iopub.execute_input":"2023-03-12T19:49:12.730474Z","iopub.status.idle":"2023-03-12T19:49:12.749905Z","shell.execute_reply.started":"2023-03-12T19:49:12.730441Z","shell.execute_reply":"2023-03-12T19:49:12.748940Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                       response  \\\n614                   This says platform 9 3/4.   \n449  But, Hagrid, how am I to pay for all this?   \n631                       Excuse me. Excuse me.   \n821                          I don't know, Sir.   \n536                          You know, Hagrid.    \n\n                                               context  \\\n614              But, Hagrid, there must be a mistake.   \n449                        It's the fastest model yet.   \n631                                             Sorry.   \n821  And what is the difference between monkshood a...   \n536                          The one who gave me this.   \n\n                            context/0  \\\n614                   Platform 9 3/4?   \n449  Look at it! The new Nimbus 2000!   \n631  There's no such thing, is there?   \n821                I don't know, Sir.   \n536  He killed my parents, didn't he?   \n\n                                             context/1  \\\n614                              Stick to your ticket.   \n449                   It's a world-class racing broom.   \n631                          This says platform 9 3/4.   \n821  Where, Mr Potter, would you look if I asked yo...   \n536                               You seem very quiet.   \n\n                                             context/2  \\\n614         Stick to it, Harry that's very important.    \n449  And over there, all your bits and bobs for doi...   \n631              But, Hagrid, there must be a mistake.   \n821             You don't know? Well, let's try again.   \n536                              You all right, Harry?   \n\n                                             context/3  \\\n614                               Here's your ticket.    \n449  Here's where you'll get your quills and your ink.   \n631                                    Platform 9 3/4?   \n821  Tell me, what would I get if I added powdered ...   \n536                                    Happy birthday.   \n\n                                     context/4  \\\n614  Now, uh, your train leaves in 10 minutes.   \n449           Welcome, Harry, to Diagon Alley.   \n631                      Stick to your ticket.   \n821                         Our new celebrity.   \n536                              Harry! Harry!   \n\n                                             context/5  \n614                  Well, he'll be wanting to see me.  \n449  I'm not sure I'm exactly the right person to t...  \n631         Stick to it, Harry that's very important.   \n821                                        Mr. Potter.  \n536                            Terrible yes, but great  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>614</th>\n      <td>This says platform 9 3/4.</td>\n      <td>But, Hagrid, there must be a mistake.</td>\n      <td>Platform 9 3/4?</td>\n      <td>Stick to your ticket.</td>\n      <td>Stick to it, Harry that's very important.</td>\n      <td>Here's your ticket.</td>\n      <td>Now, uh, your train leaves in 10 minutes.</td>\n      <td>Well, he'll be wanting to see me.</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>But, Hagrid, how am I to pay for all this?</td>\n      <td>It's the fastest model yet.</td>\n      <td>Look at it! The new Nimbus 2000!</td>\n      <td>It's a world-class racing broom.</td>\n      <td>And over there, all your bits and bobs for doi...</td>\n      <td>Here's where you'll get your quills and your ink.</td>\n      <td>Welcome, Harry, to Diagon Alley.</td>\n      <td>I'm not sure I'm exactly the right person to t...</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>Excuse me. Excuse me.</td>\n      <td>Sorry.</td>\n      <td>There's no such thing, is there?</td>\n      <td>This says platform 9 3/4.</td>\n      <td>But, Hagrid, there must be a mistake.</td>\n      <td>Platform 9 3/4?</td>\n      <td>Stick to your ticket.</td>\n      <td>Stick to it, Harry that's very important.</td>\n    </tr>\n    <tr>\n      <th>821</th>\n      <td>I don't know, Sir.</td>\n      <td>And what is the difference between monkshood a...</td>\n      <td>I don't know, Sir.</td>\n      <td>Where, Mr Potter, would you look if I asked yo...</td>\n      <td>You don't know? Well, let's try again.</td>\n      <td>Tell me, what would I get if I added powdered ...</td>\n      <td>Our new celebrity.</td>\n      <td>Mr. Potter.</td>\n    </tr>\n    <tr>\n      <th>536</th>\n      <td>You know, Hagrid.</td>\n      <td>The one who gave me this.</td>\n      <td>He killed my parents, didn't he?</td>\n      <td>You seem very quiet.</td>\n      <td>You all right, Harry?</td>\n      <td>Happy birthday.</td>\n      <td>Harry! Harry!</td>\n      <td>Terrible yes, but great</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Create a dataset suitable for our model\ndef construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                self.examples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.751153Z","iopub.execute_input":"2023-03-12T19:49:12.752414Z","iopub.status.idle":"2023-03-12T19:49:12.764935Z","shell.execute_reply.started":"2023-03-12T19:49:12.752385Z","shell.execute_reply":"2023-03-12T19:49:12.763975Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Caching and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.766860Z","iopub.execute_input":"2023-03-12T19:49:12.767731Z","iopub.status.idle":"2023-03-12T19:49:12.780186Z","shell.execute_reply.started":"2023-03-12T19:49:12.767671Z","shell.execute_reply":"2023-03-12T19:49:12.779223Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Build the model\nfrom transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:12.785808Z","iopub.execute_input":"2023-03-12T19:49:12.786636Z","iopub.status.idle":"2023-03-12T19:49:19.676560Z","shell.execute_reply.started":"2023-03-12T19:49:12.786599Z","shell.execute_reply":"2023-03-12T19:49:19.675458Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510ed798b68b484198a4f7b3db9c5c1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7833df7455ae4f41996cf2a96d27df5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b50390c12542ee97867204788499ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912833de03654676ad1dc28e193f282e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:1252: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27858b5c93c546d9a49621881f8c7b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c96b82a06e3d49a69c015bd3d0a840cb"}},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\n# Configurations\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.678340Z","iopub.execute_input":"2023-03-12T19:49:19.679095Z","iopub.status.idle":"2023-03-12T19:49:19.686256Z","shell.execute_reply.started":"2023-03-12T19:49:19.679055Z","shell.execute_reply":"2023-03-12T19:49:19.685189Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Arguments to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 50\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.687969Z","iopub.execute_input":"2023-03-12T19:49:19.688419Z","iopub.status.idle":"2023-03-12T19:49:19.699984Z","shell.execute_reply.started":"2023-03-12T19:49:19.688327Z","shell.execute_reply":"2023-03-12T19:49:19.698746Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train the model\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n     # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n    \n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n                    if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                        checkpoint_prefix = \"checkpoint\"\n                        # Save model checkpoint\n                        output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                        os.makedirs(output_dir, exist_ok=True)\n                        model_to_save = (\n                             model.module if hasattr(model, \"module\") else model\n                        )  # Take care of distributed/parallel training\n                        model_to_save.save_pretrained(output_dir)\n                        tokenizer.save_pretrained(output_dir)\n\n                        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                        _rotate_checkpoints(args, checkpoint_prefix)\n\n                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n                    print(global_step)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.701865Z","iopub.execute_input":"2023-03-12T19:49:19.702339Z","iopub.status.idle":"2023-03-12T19:49:19.737836Z","shell.execute_reply.started":"2023-03-12T19:49:19.702301Z","shell.execute_reply":"2023-03-12T19:49:19.736622Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.739349Z","iopub.execute_input":"2023-03-12T19:49:19.739673Z","iopub.status.idle":"2023-03-12T19:49:19.755326Z","shell.execute_reply.started":"2023-03-12T19:49:19.739637Z","shell.execute_reply":"2023-03-12T19:49:19.754280Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Main runner\n\ndef main(df_trn, df_val):\n    args = Args()\n    \n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelWithLMHead.from_pretrained(\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n    \n    logger.info(\"Training/evaluation parameters %s\", args)\n    \n# Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n        \n# Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.757129Z","iopub.execute_input":"2023-03-12T19:49:19.757483Z","iopub.status.idle":"2023-03-12T19:49:19.777324Z","shell.execute_reply.started":"2023-03-12T19:49:19.757447Z","shell.execute_reply":"2023-03-12T19:49:19.776362Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"main(trn_df, val_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T19:49:19.778810Z","iopub.execute_input":"2023-03-12T19:49:19.779837Z","iopub.status.idle":"2023-03-12T20:22:25.197575Z","shell.execute_reply.started":"2023-03-12T19:49:19.779799Z","shell.execute_reply":"2023-03-12T20:22:25.196556Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e163619665477caa3009ef8496afcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2807669259471ca586e6a689c87c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24da27c7cc3740dfb334723c45209604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e053b2ebf12340f6a33ca377d1359d0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a050f70e4e2b463393c3c198b393af9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe514b309bf4481b2cd3832b35f9bc6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec007123ee6f48be96f096121c5ef7d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8e16abec3f4d4191cb3af11af4cd9f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a850facf96bc4b7eaef8813192bcaadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c9ae6c68b84fd38680b261ad82ccbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ccf72bc3d60413a9a71b7c9345c86e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82cab44da12f4e29bda5048d468930b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e089f4f676b74b5cb2331691957f4351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fe9f0822634f23bd1d010594c4d948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df217eee702144479c3692e610f1cd13"}},"metadata":{}},{"name":"stdout","text":"1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f114353c8644e69b0bf53252a40750e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e38d5f411f545c1bb589023cab32434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fded6a2134b4ad482a330a7381af63e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4346c1f74ad54acf8acc09933c3ca93b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47319892bde643ae8528ed72dc8edd0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb792bed6b294526a7a2acf8a6443cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fb51ab7f194c56917b02e94a1f856c"}},"metadata":{}},{"name":"stdout","text":"2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bce9a95e70345339533c7fffcc8df40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf81fe3f08244b29ce19ff357993336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33878839af3243f1a664402b0a6742b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e773cce766574b35b948bbf3beb9d4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a249ebea638145ac80f71b636c6d3b4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6131e0079dea45ec9856a48a2cbb117f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e573ec7aef4938a125c8d9ee119062"}},"metadata":{}},{"name":"stdout","text":"3000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ab35fc411540838ad793e67b4d3ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f540d654f9404093e96e2ca0d35a5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b4e308faea4baaa944fbef5a8a41aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2423bf76637948e7a76f3392606a4b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a2bfc38ca44e57a744b5bb97efc3ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf2011f622145cbb6eccbbf204ad2c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e61f882ad6041229d64264333ccbbe6"}},"metadata":{}},{"name":"stdout","text":"4000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4998c979dd204fcb993373466f48203d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09456c2c4f843c1803dc75380f169d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05df3c6e63714e918dc665910d85f3e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8024dcce735c4e27bfd29e2f4f0dd8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f3d38e53e04a6886b4a9fbfe6f7d95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c460d506b8f415bbf00fc2bea5d3ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f223ccb2017640f09725cb50e9f293bf"}},"metadata":{}},{"name":"stdout","text":"5000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d316c57b864b758893c12a7274394b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dbd3cec93d463a8b3790bf5b5178b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7fcced404934dbeb3b495c3f30e78b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a46c83e657ea49689126f0cb60e8f6d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ce45fc29bc4338973bae5228c8ffbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76be2841fb3c405095059e3accd8ae13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ffc63eb2f544cc947fafe113602cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727836098899465d92fe8fce284152ee"}},"metadata":{}},{"name":"stdout","text":"6000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc04a414ef8c4053a0f15997565a3500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2f8bbb8a7448e6a3362f91b196d4a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7d7c60df5f4b4fb356c44ee2333652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec61fc82e9e84314935e51cd201819f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc02caefb99a4d9a88bd703b19fcbb6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/139 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e504e3b12a0456fb0361995c40a6819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60fef798491a4cd1a9c2107750634cd2"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'perplexity_': tensor(1.0099)}"},"metadata":{}}]},{"cell_type":"code","source":"# Load the trained model\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('output-small')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:36:09.924911Z","iopub.execute_input":"2023-03-12T20:36:09.925767Z","iopub.status.idle":"2023-03-12T20:36:12.136636Z","shell.execute_reply.started":"2023-03-12T20:36:09.925720Z","shell.execute_reply":"2023-03-12T20:36:12.135597Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\")+tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    # pretty print last ouput tokens from bot\n    print(\"Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:42:34.129829Z","iopub.execute_input":"2023-03-12T20:42:34.130775Z","iopub.status.idle":"2023-03-12T20:43:17.140465Z","shell.execute_reply.started":"2023-03-12T20:42:34.130725Z","shell.execute_reply":"2023-03-12T20:43:17.139255Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdin","text":">> User: Good evening\n"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Bot: Now, what would three young Gryffindors such as yourselves be doing inside on a day like this?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: Sorry professor\n"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Bot: It's for your own good, you know.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: Yes, I know\n"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Bot: One minute the glass was there and then it was gone, it was like magic.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: I think we learned magic well\n"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Bot: Learnt what?\n","output_type":"stream"}]}]}